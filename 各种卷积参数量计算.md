

<h2 align='center'>卷积参数量计算 总整理</h2>

CSDN https://blog.csdn.net/weixin_44638957/article/details/105177543


<h2 align='center'>卷积参数量计算 总整理</h2>

这边其实算写给我自己复习用的， 就没有用博客的口吻了 简单为主




**预备知识**

**FLOPs：** **s小写**，指**浮点运算数**，理解为计算量。可以用来衡量算法/模型的复杂度。（**模型**） 在论文中常用GFLOPs（1 GFLOPs = $10^9$ FLOPs）相当于十亿次

为了理解下面的计算， 简单复习一下卷积运算的过程， 就是将kernel在原input fmap上或者原图上进行滑动(左向右）， 并且进行element wise multiplication 点乘， 然后最终将点乘后的元素进行相加 add， 也就是MAC， 这在计算量中算是两次运算

参考 

1. [轻量级神经网络“巡礼”（一）—— ShuffleNetV2](https://zhuanlan.zhihu.com/p/67009992)
2. [轻量化网络ShuffleNet MobileNet v1/v2 解析](https://zhuanlan.zhihu.com/p/35405071)





#### @普通卷积计算 

##### 计算量or运算量

标准卷积FLOPs = $(2 * C_i * K^2 -1) * H * W * C_0$

- 只考虑一个input feature map， 不考虑Batchsize

- Ci=input channel, k=kernel size, HW=output feature map size, Co=output channel.

- 2是因为一个MAC算2个operations。

- 不考虑bias时有-1，有bias时没有-1。



以上主要分为两步

第一步括号内的是计算出output fmap中一个pixel的计算量

外面的 HWC0 则是扩展到整个output feature map的步骤， 所以就是相乘



**参数量**


简单来说就是需要多少个可以学习的参数， 这里理解的思路为因为与input fmap进行卷积， 卷积核上的每一点weight都要进行学习， 所以需要学习的量为K平方， 注意这只是单个通道的卷积核需要学习的参数量， 还要依照进来的多少个通道的input， 卷积核在扩展成一样的通道数， 接着input的每个通道都与卷积核进行卷积之后， 在依照输出的通道数，就会产生几组的卷积核组， 于是进行相乘



- 输入尺寸为 $H*W*C_1$, 

- 卷积核为 $k * k * C_1$

- 输出特征图 通道数 $c_2$

- 标准卷积参数量计算为 $(k^2 * c_1) * c_2$  ， 都知道卷积核为多少个通道， 输出的特征就是多少个通道



思路一样是先计算input进来的参数量， 在依据输出的通道数进行相乘
假设输入为( 1, 3, 8, 8), 经过conv(2, 1, 1)卷积
则该op的参数量为 $(1^2 * 3) *  2 = 6$

------



#### @Group 卷积

卷积计算的部分先看参数量， 在来看计算量
##### 参数量

也就是将输入的fmap 依照自己设定的组数g， 来分组别进行计算

- Input feature map 为 H x W x C/g

- 单个卷积核大小为 K x K x C/g （很好理解， 因为input分成g组了， 卷积核也要进行分组才能分开卷积）

- 输出数据为 H2 x W2 x C2/g 

- Group 分组卷积的参数量计算方式为

$k * k * \frac{c1}{g} * \frac{c2}{g}*g$


先看前面的$k * k * \frac{c1}{g} * \frac{c2}{g}$ 其实与一般的标准卷积计算的方式差不多， 只不过这里先计算分组后单个组别的计算量， 所以可以看到是乘上 $\frac{c1}{g} * \frac{c2}{g}$
最终因为是g个组别， 所以还要乘上g组才等于完整的计算量

举个例子
Ex.
```
input (1, 64, 8, 8) 这里为了好分组， 输入channel为64
conv(2, 1, 1) 表示输出channel为2， 卷积为1x1
g  = 2表示分成2组
所以计算量为 1^2 x 64/2 x 2/2 x 2 = 64
```


##### 计算量or运算量
思路一样跟着走， 先计算出 要产出一个output pixel的计算量要多少在扩展到整个output map

$[(2 * k^2 * Ci/g * +1) * H_2 * W_2 * C_o/g] * g$

这里唯一和标准卷积不一样的地方只在于通道的地方被g给分组了





------



#### @深度可分类卷积（Depthwise separable conv）

深度可分类卷积主要是由 

- depthwise conv ： 负责滤波 尺寸为 $Dk * Dk * 1$, 共M个， （Dk 就是 Depthwise kernel size）， 作用在输入的每个通道上， 所以1表示单个通道， M就是依照输入特征的通道数分成M组
- pointwise conv :  负责转换通道， 尺寸为 $1 * 1 * M$, 共N个， 因为是做1x1的卷积， 并且要增加通道数为M， 一共有N个卷积核

##### 参数量

- depthwise conv ： $Dk * Dk * 1 * M$  
- pointwise conv :  $1 * 1 * M * N$
- 将上面两个参数量相加就是 $D_k * D_k * M  + M * N$ 

深度可卷积是标准卷积的

<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BD_K%C3%97D_K%C3%97M%2BM%C3%97N%7D%7BD_K%C3%97D_K%C3%97M%C3%97N%7D+%3D+%5Cfrac%7B1%7D%7BN%7D+%2B+%5Cfrac%7B1%7D%7BD_K%5E2%7D++">



depthwise深度卷积其实就是g = M = N 的分组卷积，但没有将g组concate起来， 所以参数量为 $1/N$

pointwise conv其实就是讲g组卷积用conv1x1拼接起来， 所以pointwise参数量是 标准卷积的 $1/D_k^2$ 

