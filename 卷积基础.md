截取自原文 https://zhuanlan.zhihu.com/p/114679988

了解完单个卷积是如何计算的之后，我们就可以从神经网络的角度来看‘卷积层’的运算过程了。下图展示的是输入三通图像（8*8*3）经一层卷积结构，输出两通特征图（8*8*2）的计算过程：

<img src='https://pic3.zhimg.com/80/v2-aaa37b579e4a2270c78eb10ebf9522b2_1440w.jpg'>

卷积参数：input_shape=(8,8,3)，kernelsize=(3,3)，padding=‘same’，stride=1，output_shape=(8,8,2)
在此图中：

A：原图（3通道，8*8）
B：展平了方便看的原图
C：不同的卷积核（以颜色和虚线类型区分）
D：不同卷积核在底图上扫过所得到的单个计算结果图（以颜色和虚线类型区分）
E：展平了方便看的输出图
F：输出图（2通道，8*8）
在此次卷积层的运算中：

首先我们来关注一下输入和输出，他俩的尺度都是（8*8），而输入是3通道，输出是2通道（深度学习中不管干啥一定要先看输入输出，对一层是这样，对整个模型也是这样）。

其次就准备进入我们最熟悉的卷积核计算了，可是在此之前我们得知道，这个运算过程中到底发生了几次卷积核计算呢？有的朋友可能要说，卷积的一大特性就是‘权值共享’，有几通输出就有几个卷积核，每个卷积核把输入特征图从头扫到尾。然而这个其实是不对的！

实际上，在卷积核计算数量问题上，应该是“有几通道的输出就有几套卷积核，每套内的卷积核数量与输入通道数相等”，就像我在上图中所画的：

由C中的上下两套，每套三个卷积核去扫输入的3张图（颜色一一对应）；
得到D中的两套，每套3张计算结果图；
在经求和及加入偏置量，得到要输出的2通道结果。
至此，这一个卷积层的运算就全部完成了。


硬件处理卷积需要的FLOPs算力(Float-point-operation per seconds)计算
https://www.thinkautonomous.ai/blog/?p=deep-learning-optimization
